{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import warnings\n",
    "from string import digits\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', index_col =\"tweet_id\")\n",
    "test = pd.read_csv('./data/test.csv', index_col =\"tweet_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      tweet  sentiment\n",
       "tweet_id                                                              \n",
       "1701      #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1851      Learning ab Google doodles! All doodles should...          1\n",
       "2689      one of the most in-your-face ex. of stealing t...          2\n",
       "4525      This iPhone #SXSW app would b pretty awesome i...          0\n",
       "3604      Line outside the Apple store in Austin waiting...          1"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7506</th>\n",
       "      <td>Audience Q: What prototyping tools do you use?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>At SXSW? Send Your Best Photos &amp;amp; Videos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>@mention  and here's a pic of you winning your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7688</th>\n",
       "      <td>Google Marissa Mayer: mobile phone as a cursor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>#SXSW Google maps is even cooler than I thought</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      tweet\n",
       "tweet_id                                                   \n",
       "7506      Audience Q: What prototyping tools do you use?...\n",
       "7992      At SXSW? Send Your Best Photos &amp; Videos to...\n",
       "247       @mention  and here's a pic of you winning your...\n",
       "7688      Google Marissa Mayer: mobile phone as a cursor...\n",
       "3294        #SXSW Google maps is even cooler than I thought"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "#     print(input_txt)\n",
    "#     print(pattern)\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for x in r:\n",
    "        input_txt = re.sub(x, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "def remove_digits(str_txt):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    str_txt = str_txt.translate(remove_digits)\n",
    "    return str_txt\n",
    "\n",
    "def data_cleaning(table):\n",
    "    #Put everything in lower case\n",
    "    table['tweet'] = table['tweet'].str.lower()\n",
    "    #Replace @<some_user>\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"@[\\w]*\"))\n",
    "    #replace '#'\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"#[\\w]*\"))\n",
    "    #Replace RT:rt\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"rt\"))\n",
    "    #Remove URL's\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"r'^https?:\\/\\/.*[\\r\\n]*'\"))\n",
    "    #Remove Numbers\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_digits(row))\n",
    "    #Remove Special Characters & Punctuantions\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"r'[!$%&()*+,-./:;<=>?@[\\]^_`{|}~]'\"))\n",
    "    table['tweet'] = table['tweet'].apply(lambda row:remove_pattern(row, \"r'#([^\\s]+)', r'\\1'\"))\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Savio\n",
      "[nltk_data]     Coelho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Savio\n",
      "[nltk_data]     Coelho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Savio\n",
      "[nltk_data]     Coelho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Savio Coelho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import (\n",
    "    wordnet,\n",
    "    stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Null rows if any\n",
    "def remove_na(table):\n",
    "    table = table.dropna()\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the data\n",
    "def data_tokenize(table):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    table['tweet_tokenize'] = table['tweet'].apply(lambda row:tokenizer.tokenize(row))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Removing Stop Words\n",
    "def remove_stop_words(table):\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    table['tweet_tokenize'] = table['tweet_tokenize'].apply(lambda x:[word for word in x if not word in en_stop] )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing the words\n",
    "def lemmatize_words(table):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    table['tweet_tokenize'] = table['tweet_tokenize'].apply(lambda x: [lemma.lemmatize(word) for word in x])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_dict(word):\n",
    "    if wordnet.synsets(word):\n",
    "        #if the word is in the dictionary, we'll return True\n",
    "        return True\n",
    "\n",
    "def replace_elongated_word(word):\n",
    "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
    "    repl = r'\\1\\2\\3'    \n",
    "    if in_dict(word):\n",
    "        return word\n",
    "    new_word = re.sub(regex, repl, word)\n",
    "    if new_word != word:\n",
    "        return replace_elongated_word(new_word)\n",
    "    else:\n",
    "        return new_word\n",
    "\n",
    "def detect_elongated_words(row):\n",
    "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
    "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
    "    for word in words:\n",
    "        if not in_dict(word):\n",
    "            row = re.sub(word, replace_elongated_word(word), row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing and replacing contractions eg. wasn't, couldn't, shouldn't....etc\n",
    "import contractions\n",
    "def remove_replace_contractions(row):\n",
    "    str_fixed = contractions.fix(row)\n",
    "    return str_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = remove_na(train)\n",
    "train = data_cleaning(train)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: detect_elongated_words(x))\n",
    "train['tweet'] = train['tweet'].apply(lambda x: remove_replace_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_tokenize(train)\n",
    "train = remove_stop_words(train)\n",
    "train = lemmatize_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_tokenize</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>defining language of touch with different d...</td>\n",
       "      <td>1</td>\n",
       "      <td>[defining, language, touch, different, dialect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>learning ab google doodles! all doodles should...</td>\n",
       "      <td>1</td>\n",
       "      <td>[learning, ab, google, doodle, doodle, light, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "      <td>[one, face, ex, stealing, show, yr, quot, appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>this iphone  ap would b pretty awesome if it d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[iphone, ap, would, b, pretty, awesome, crash,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>line outside the apple store in austin waiting...</td>\n",
       "      <td>1</td>\n",
       "      <td>[line, outside, apple, store, austin, waiting,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      tweet  sentiment  \\\n",
       "tweet_id                                                                 \n",
       "1701         defining language of touch with different d...          1   \n",
       "1851      learning ab google doodles! all doodles should...          1   \n",
       "2689      one of the most in-your-face ex. of stealing t...          2   \n",
       "4525      this iphone  ap would b pretty awesome if it d...          0   \n",
       "3604      line outside the apple store in austin waiting...          1   \n",
       "\n",
       "                                             tweet_tokenize  \n",
       "tweet_id                                                     \n",
       "1701      [defining, language, touch, different, dialect...  \n",
       "1851      [learning, ab, google, doodle, doodle, light, ...  \n",
       "2689      [one, face, ex, stealing, show, yr, quot, appl...  \n",
       "4525      [iphone, ap, would, b, pretty, awesome, crash,...  \n",
       "3604      [line, outside, apple, store, austin, waiting,...  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet               sxsw recap day  it is great? exploring brand...\n",
       "sentiment                                                         1\n",
       "tweet_tokenize    [sxsw, recap, day, great, exploring, brand, ma...\n",
       "Name: 6705, dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[6705]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
